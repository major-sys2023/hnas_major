{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-11T06:16:42.604088Z",
     "iopub.status.busy": "2023-03-11T06:16:42.603714Z",
     "iopub.status.idle": "2023-03-11T06:16:47.011889Z",
     "shell.execute_reply": "2023-03-11T06:16:47.010817Z",
     "shell.execute_reply.started": "2023-03-11T06:16:42.604061Z"
    },
    "id": "5JKFOepBjjII"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout, Flatten,GlobalMaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import BatchNormalization\n",
    "# from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "# from keras.layers import LeakyReLU \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-11T06:16:47.014805Z",
     "iopub.status.busy": "2023-03-11T06:16:47.013771Z",
     "iopub.status.idle": "2023-03-11T06:16:49.102337Z",
     "shell.execute_reply": "2023-03-11T06:16:49.101312Z",
     "shell.execute_reply.started": "2023-03-11T06:16:47.014747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1282 files belonging to 2 classes.\n",
      "Using 1026 files for training.\n",
      "Found 1282 files belonging to 2 classes.\n",
      "Using 256 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_dir=\"/notebooks/thermograms/Desenvolvimento da Metodologia\"\n",
    "validation_dir=train_dir\n",
    "batch_size=16\n",
    "test_train_split=0.2\n",
    "train_data = image_dataset_from_directory(\\\n",
    "      train_dir,color_mode=\"grayscale\",image_size=(227,227) ,\\\n",
    "      subset='training',seed=12, validation_split=test_train_split,\\\n",
    "      batch_size=batch_size)\n",
    "validation_data = image_dataset_from_directory(validation_dir,\n",
    "      color_mode=\"grayscale\",image_size=(227,227), subset='validation',seed=12,\\\n",
    "      validation_split=test_train_split,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-11T06:16:49.103724Z",
     "iopub.status.busy": "2023-03-11T06:16:49.103497Z",
     "iopub.status.idle": "2023-03-11T06:16:49.111338Z",
     "shell.execute_reply": "2023-03-11T06:16:49.110488Z",
     "shell.execute_reply.started": "2023-03-11T06:16:49.103701Z"
    },
    "id": "R8-cow2LjlLo"
   },
   "outputs": [],
   "source": [
    "def build_model(layer_dims, input_shape=(227,227,3,),len_classes=3, dropout_rate=0.2,activation='relu'):\n",
    "    \"\"\"Function to build a model with specified layer dimensions and activation function.\"\"\"\n",
    "    model = Sequential()\n",
    "    for i, dim in enumerate(layer_dims):\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(dim[0], dim[1], input_shape=input_shape, activation=activation))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        else:\n",
    "            model.add(Conv2D(dim[0], dim[1], activation=activation))\n",
    "            if True:#i%2!=0:\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(filters=, kernel_size=1, strides=1))\n",
    "        #model.add(Dropout(dropout_rate))\n",
    "        # model.add(BatchNormalization())\n",
    "        #model.add(GlobalMaxPooling2D())\n",
    "        #model.add(Activation('sigmoid'))\n",
    "    # model.add(Dropout(dropout_rate))\n",
    "    # Output Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(len_classes-1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-11T06:16:49.114222Z",
     "iopub.status.busy": "2023-03-11T06:16:49.113597Z",
     "iopub.status.idle": "2023-03-11T06:16:49.119023Z",
     "shell.execute_reply": "2023-03-11T06:16:49.118180Z",
     "shell.execute_reply.started": "2023-03-11T06:16:49.114196Z"
    },
    "id": "WMfe0TL_joQp"
   },
   "outputs": [],
   "source": [
    "def sample_architecture(min_layers=1, max_layers=5, min_filters=32, max_filters=512, min_kernel=3, max_kernel=5):\n",
    "    \"\"\"Function to sample a random architecture from the search space.\"\"\"\n",
    "    num_layers = random.randint(min_layers, max_layers)\n",
    "    layer_dims = [(random.randint(min_filters, max_filters), random.randint(min_kernel, max_kernel)) for _ in range(num_layers)]\n",
    "    return layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-11T06:16:49.120121Z",
     "iopub.status.busy": "2023-03-11T06:16:49.119898Z",
     "iopub.status.idle": "2023-03-11T06:16:49.125761Z",
     "shell.execute_reply": "2023-03-11T06:16:49.124676Z",
     "shell.execute_reply.started": "2023-03-11T06:16:49.120100Z"
    },
    "id": "vekdUB7jjq2S"
   },
   "outputs": [],
   "source": [
    "def mutate_architecture(layer_dims, mutation_rate=0.1, min_layers=1, max_layers=5, min_filters=32, max_filters=512, min_kernel=3, max_kernel=5):\n",
    "    \"\"\"Function to mutate an architecture by randomly modifying some of its layer dimensions.\"\"\"\n",
    "    num_layers = len(layer_dims)\n",
    "    for i in range(num_layers):\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            layer_dims[i] = (random.randint(min_filters, max_filters), random.randint(min_kernel, max_kernel))\n",
    "    return layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-11T06:16:49.127388Z",
     "iopub.status.busy": "2023-03-11T06:16:49.127068Z",
     "iopub.status.idle": "2023-03-11T06:16:49.133120Z",
     "shell.execute_reply": "2023-03-11T06:16:49.132299Z",
     "shell.execute_reply.started": "2023-03-11T06:16:49.127363Z"
    },
    "id": "6-jGT9t8RZsa"
   },
   "outputs": [],
   "source": [
    "def breed_architectures(parent1, parent2, mutation_rate, min_layers, max_layers, min_filters, max_filters, min_kernel, max_kernel):\n",
    "    \"\"\"Function to breed two parent architectures to produce a child architecture.\"\"\"\n",
    "    child = []\n",
    "    for i in range(len(parent1)):\n",
    "        if np.random.rand() < 0.5:\n",
    "            child.append(parent1[i])\n",
    "        else:\n",
    "            if i < len(parent2):\n",
    "                child.append(parent2[i])\n",
    "    return mutate_architecture(child, mutation_rate, min_layers, max_layers, min_filters, max_filters, min_kernel, max_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-11T06:16:49.134411Z",
     "iopub.status.busy": "2023-03-11T06:16:49.134157Z",
     "iopub.status.idle": "2023-03-11T06:16:49.141190Z",
     "shell.execute_reply": "2023-03-11T06:16:49.140445Z",
     "shell.execute_reply.started": "2023-03-11T06:16:49.134387Z"
    },
    "id": "lbnz3OeokBgR"
   },
   "outputs": [],
   "source": [
    "#Train the model on the dataset and evaluate its performance.\n",
    "def train_and_evaluate_model(model,epochs=10, train_dir=None,X_train=None, y_train=None,\\\n",
    "                             X_test=None,y_test=None):\n",
    "    \"\"\"Function to train and evaluate a model.\"\"\"\n",
    "    if train_dir is not None:\n",
    "        #train_data,validation_data=get_data(train_dir,train_dir)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer='Adam', metrics=[\"BinaryAccuracy\"])\n",
    "        # print(model.summary())\n",
    "        history = model.fit(train_data, epochs=epochs, verbose=1,validation_data=validation_data)\n",
    "        # return history\n",
    "        # print(history.history.keys())\n",
    "        # Extract the accuracy from the history object\n",
    "        acc = history.history['val_binary_accuracy'][len(history.history['val_binary_accuracy'])-1]\n",
    "        return acc\n",
    "    else:\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer='Adam', metrics=[\"BinaryAccuracy\"])\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=1)\n",
    "        scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "        return scores[1]  # Return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-11T06:16:49.142847Z",
     "iopub.status.busy": "2023-03-11T06:16:49.142577Z",
     "iopub.status.idle": "2023-03-11T06:16:49.155507Z",
     "shell.execute_reply": "2023-03-11T06:16:49.154480Z",
     "shell.execute_reply.started": "2023-03-11T06:16:49.142823Z"
    },
    "id": "2zwUTo9ykIp7"
   },
   "outputs": [],
   "source": [
    "def genetic_algorithm(train_dir, epochs, population_size=20, len_classes=3, num_generations=10, mutation_rate=0.1,\\\n",
    "                      min_layers=1, max_layers=5, min_filters=32, max_filters=512,\\\n",
    "                      min_kernel=3, max_kernel=5, save_after=10,checkpoint_file=None):\n",
    "    \"\"\"Function to implement the genetic algorithm to evolve the architecture.\"\"\"\n",
    "    if checkpoint_file and os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            population = data['population']\n",
    "            scores = data['scores']\n",
    "            start_generation = data['generation']\n",
    "            start_population = data['population_id']\n",
    "    else:\n",
    "        # Initialize population\n",
    "        population = [sample_architecture(min_layers, max_layers, min_filters,\\\n",
    "                                          max_filters,min_kernel, max_kernel)\\\n",
    "                      for _ in range(population_size)]\n",
    "        scores = []\n",
    "        start_generation = 0\n",
    "        start_population = 0\n",
    "    \n",
    "    for i in range(start_generation, num_generations):\n",
    "        print(\"Generation {}/{}\".format(i + 1, num_generations))\n",
    "        new_population = []\n",
    "        for j in range(population_size):\n",
    "            # Select two random parents\n",
    "            the_choice=np.random.choice(population_size, 2, replace=False)\n",
    "            parent1=population[the_choice[0]]\n",
    "            parent2=population[the_choice[1]]\n",
    "            # Breed the parents to produce a child\n",
    "            child = breed_architectures(parent1, parent2, mutation_rate,\\\n",
    "                                        min_layers,max_layers, min_filters,\\\n",
    "                                        max_filters, min_kernel, max_kernel)\n",
    "            new_population.append(child)\n",
    "            # Checkpoint after every 5th model trained\n",
    "            if checkpoint_file and j % save_after == save_after-1:\n",
    "                data = {\n",
    "                    'population': population,\n",
    "                    'scores': scores,\n",
    "                    'generation': i,\n",
    "                    'population_id': start_population+j+1\n",
    "                }\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "        # Evaluate each model in the population\n",
    "        generation_scores = []\n",
    "        for j, layer_dims in enumerate(new_population):\n",
    "            print(str(j)+\"/\"+str(population_size))            \n",
    "            model = build_model(layer_dims,len_classes=len_classes,input_shape=(227,227,1,))\n",
    "            score = train_and_evaluate_model(model,epochs=epochs,train_dir=train_dir)\n",
    "            generation_scores.append((start_population+j, layer_dims, score))\n",
    "            # Checkpoint after each model trained\n",
    "            if checkpoint_file:\n",
    "                data = {\n",
    "                    'population': population,\n",
    "                    'scores': scores + generation_scores[-5:],\n",
    "                    'generation': i,\n",
    "                    'population_id': start_population+j+1\n",
    "                }\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "        # Sort the models by accuracy and select the best ones for the next generation\n",
    "        generation_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "        population = [x[1] for x in generation_scores[:population_size]]\n",
    "        scores.extend(generation_scores)\n",
    "        # Save checkpoint after each generation\n",
    "        if checkpoint_file:\n",
    "            data = {\n",
    "                'population': population,\n",
    "                'scores': scores,\n",
    "                'generation': i,\n",
    "                'population_id': start_population+population_size\n",
    "            }\n",
    "            with open(checkpoint_file, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "\n",
    "    # Return the best architecture\n",
    "    return population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-11T07:11:14.179682Z",
     "iopub.status.busy": "2023-03-11T07:11:14.179368Z",
     "iopub.status.idle": "2023-03-11T07:11:51.934580Z",
     "shell.execute_reply": "2023-03-11T07:11:51.932609Z",
     "shell.execute_reply.started": "2023-03-11T07:11:14.179657Z"
    },
    "id": "1fn6mVEWSatF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3/100\n",
      "0/50\n",
      "Epoch 1/7\n",
      "65/65 [==============================] - 22s 286ms/step - loss: 9.7399 - binary_accuracy: 0.8402 - val_loss: 0.2737 - val_binary_accuracy: 0.9375\n",
      "Epoch 2/7\n",
      "55/65 [========================>.....] - ETA: 2s - loss: 0.0905 - binary_accuracy: 0.9614"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_architecture2 \u001b[38;5;241m=\u001b[39m \u001b[43mgenetic_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_generations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutation_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmin_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmin_kernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_kernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/notebooks/hnas_major/models/checkpoint_file.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [8], line 49\u001b[0m, in \u001b[0;36mgenetic_algorithm\u001b[0;34m(train_dir, epochs, population_size, len_classes, num_generations, mutation_rate, min_layers, max_layers, min_filters, max_filters, min_kernel, max_kernel, save_after, checkpoint_file)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(j)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(population_size))            \n\u001b[1;32m     48\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(layer_dims,len_classes\u001b[38;5;241m=\u001b[39mlen_classes,input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m227\u001b[39m,\u001b[38;5;241m227\u001b[39m,\u001b[38;5;241m1\u001b[39m,))\n\u001b[0;32m---> 49\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m generation_scores\u001b[38;5;241m.\u001b[39mappend((start_population\u001b[38;5;241m+\u001b[39mj, layer_dims, score))\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Checkpoint after each model trained\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [7], line 9\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, epochs, train_dir, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinaryAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(model.summary())\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# return history\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(history.history.keys())\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Extract the accuracy from the history object\u001b[39;00m\n\u001b[1;32m     13\u001b[0m acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_binary_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mlen\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_binary_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_architecture2 = genetic_algorithm(train_dir=train_dir,len_classes=2,epochs=7,population_size=50, num_generations=100, mutation_rate=0.1,\\\n",
    "                      min_layers=1, max_layers=5, min_filters=32, max_filters=512,\\\n",
    "                      min_kernel=3, max_kernel=5, checkpoint_file='/notebooks/hnas_major/models/checkpoint_file.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-11T06:57:31.969335Z",
     "iopub.status.idle": "2023-03-11T06:57:31.969732Z",
     "shell.execute_reply": "2023-03-11T06:57:31.969580Z",
     "shell.execute_reply.started": "2023-03-11T06:57:31.969560Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model2=build_model(best_architecture2,len_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-11T06:57:31.970654Z",
     "iopub.status.idle": "2023-03-11T06:57:31.970978Z",
     "shell.execute_reply": "2023-03-11T06:57:31.970824Z",
     "shell.execute_reply.started": "2023-03-11T06:57:31.970808Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model2.compile(loss=\"binary_crossentropy\", optimizer='Adam', metrics=[\"BinaryAccuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-11T06:57:31.971684Z",
     "iopub.status.idle": "2023-03-11T06:57:31.972007Z",
     "shell.execute_reply": "2023-03-11T06:57:31.971850Z",
     "shell.execute_reply.started": "2023-03-11T06:57:31.971834Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model2.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
